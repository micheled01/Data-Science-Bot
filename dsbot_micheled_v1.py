# -*- coding: utf-8 -*-
"""DSBot_MicheleD_v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qoYRxl4kBfLeWaX_ZZYCrVKaMHgiczYe

Analysis considering only numerical features
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive/My Drive/Data Science ChatBot

import seaborn as sns
import pandas as pd
from sklearn.model_selection import train_test_split 
import matplotlib.pyplot as plt
import numpy as np
from sklearn.decomposition import PCA
import plotly.express as px
from scipy.stats import shapiro, kstest, anderson

df = pd.read_csv('train.csv', sep = ',')
df = df.dropna()
y = df['Activity']
dff = df.drop(columns='Activity', axis=1)
X= dff.select_dtypes(exclude=['int64'])       #consider only numerical

"""# Correlation"""

corr_matrix = X.corr()
correlations = corr_matrix.abs()
upper_triangle = correlations.where(np.triu(np.ones(correlations.shape), k=1).astype(bool))
features_correlations = upper_triangle.stack()[upper_triangle.stack() > 0.9].index.tolist()

print('There are', len(features_correlations), 'high correlations:')
for corr in features_correlations:
    print(corr[0], corr[1], (corr_matrix[corr[0]].loc[corr[1]]).round(4))

sns.clustermap(X.corr(), annot=False, cmap='coolwarm')
plt.show()

sns.heatmap(X.corr(), annot=False, cmap='coolwarm')
plt.show()

"""# Normality Test"""

results = {}

for column in X.columns:
    stat_s, p_s = shapiro(X[column])        
    stat_k, p_k = kstest(X[column],  'norm')
    results[column] = {"shaprio": stat_s, "p-value Shapiro": p_s, "Kolmogorov-Smirnov": stat_k, "p-value Kolmogorv": p_k}

stats = pd.DataFrame(results).transpose()

stats.head(10)

X_cbrt = np.cbrt(X)

results_cbrt = {}

for column in X_cbrt.columns:
    stat_s, p_s = shapiro(X_cbrt[column])
    stat_k, p_k = kstest(X_cbrt[column],  'norm')
    results_cbrt[column] = {"shapiro": stat_s, "p-value Shapiro": p_s, "Kolmogorov-Smirnov": stat_k, "p-value Kolmogorv": p_k}

stats_cbrt = pd.DataFrame(results_cbrt).transpose()

stats_cbrt.head(10)

"""# PCA"""

n_components = 5

pca = PCA(n_components=n_components)
components = pca.fit_transform(X)

total_var = pca.explained_variance_ratio_.sum() * 100

labels = {str(i): f"PC {i+1}" for i in range(n_components)}
labels['color'] = 'Target'

fig = px.scatter_matrix(
    components,
    color=y,
    dimensions=range(n_components),
    labels=labels,
    title=f'Total Explained Variance: {total_var:.2f}%',
)
fig.update_traces(diagonal_visible=False)
fig.show()

pca = PCA(n_components=3)
components = pca.fit_transform(X)

total_var = pca.explained_variance_ratio_.sum() * 100

fig = px.scatter_3d(
    components, x=0, y=1, z=2, color=y,
    title=f'Total Explained Variance: {total_var:.2f}%',
    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}
)
fig.show()

pca = PCA()
pca.fit(X)
exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)

px.area(
    x=range(1, exp_var_cumul.shape[0] + 1),
    y=exp_var_cumul,
    labels={"x": "# Components", "y": "Explained Variance"}
)

principal_components = pca.transform(X)

column_names = [f"PC {i+1}" for i in range(principal_components.shape[1])]
X_pca = pd.DataFrame(principal_components, columns=column_names)

X_pca.to_csv('principal_components.csv', index=False)

"""# Feature Selection"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif

X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)

n_features = [942, 900, 800, 700, 600, 300, 100, 50, 10]

"""**Lasso**

"""

lasso = LassoCV(max_iter = 10000, n_jobs = -1)
lasso.fit(X_train, y_train)
coef = pd.Series(lasso.coef_, index=X_pca.columns)
coef_sorted = coef.abs().sort_values(ascending=False)

coef_df = abs(lasso.coef_)
coef_lasso_df = pd.DataFrame({'Feature Name': X_pca.columns, 'Coefficients': coef_df})
coef_lasso_df = coef_lasso_df.sort_values('Coefficients', ascending=False)

coef_lasso_df

selected_features = list(X_train.columns[lasso.coef_ != 0])
X_train_lasso = X_train[selected_features]
X_test_lasso = X_test[selected_features]

svm = SVC()
svm.fit(X_train_lasso, y_train)
svm_score = svm.score(X_test_lasso, y_test)
print("SVM accuracy with", np.shape(selected_features)[0], "features given by Lasso:", svm_score)

lr = LogisticRegression(max_iter=10000)
lr.fit(X_train_lasso, y_train)
lr_score = lr.score(X_test_lasso, y_test)
print("Logistic Regression accuracy with", np.shape(selected_features)[0], "features given by Lasso:", lr_score)

for k in n_features:
    top_k_features = coef_sorted[:k].index.tolist()
    selected_features_train = X_train[top_k_features]
    selected_features_test = X_test[top_k_features]
    svm = SVC()
    svm.fit(selected_features_train, y_train)
    svm_score = svm.score(selected_features_test, y_test)
    print("SVM accuracy with", k, "features:", svm_score)

print()
print("###############")
print()

for k in n_features:
    top_k_features = coef_sorted[:k].index.tolist()
    selected_features_train = X_train[top_k_features]
    selected_features_test = X_test[top_k_features]
    lr = LogisticRegression(max_iter=10000)
    lr.fit(selected_features_train, y_train)
    lr_score = lr.score(selected_features_test, y_test)
    print("Logistic Regression accuracy with", k, "features:", lr_score)

"""**Ridge**"""

ridge = RidgeCV()
ridge.fit(X_train, y_train)
coef = pd.Series(ridge.coef_, index=X_pca.columns)
coef_sorted = coef.abs().sort_values(ascending=False)

coef_df = abs(ridge.coef_)
coef_ridge_df = pd.DataFrame({'Feature Name': X_pca.columns, 'Coefficients': coef_df})
coef_ridge_df = coef_ridge_df.sort_values('Coefficients', ascending=False)

coef_ridge_df

for k in n_features:
    top_k_features = coef_sorted[:k].index.tolist()
    selected_features_train = X_train[top_k_features]
    selected_features_test = X_test[top_k_features]
    svm = SVC()
    svm.fit(selected_features_train, y_train)
    svm_score = svm.score(selected_features_test, y_test)
    print("SVM accuracy with", k, "features:", svm_score)

print()
print("###############")
print()

for k in n_features:
    top_k_features = coef_sorted[:k].index.tolist()
    selected_features_train = X_train[top_k_features]
    selected_features_test = X_test[top_k_features]
    lr = LogisticRegression(max_iter=10000)
    lr.fit(selected_features_train, y_train)
    lr_score = lr.score(selected_features_test, y_test)
    print("Logistic Regression accuracy with", k, "features:", lr_score)

"""**ElastcNet**"""

elasticnet = ElasticNetCV(max_iter = 10000, n_jobs = -1)
elasticnet.fit(X_train, y_train)
coef = pd.Series(elasticnet.coef_, index=X_pca.columns)
coef_sorted = coef.abs().sort_values(ascending=False)

coef_df = abs(elasticnet.coef_)
coef_elasticnet_df = pd.DataFrame({'Feature Name': X_pca.columns, 'Coefficients': coef_df})
coef_elasticnet_df = coef_elasticnet_df.sort_values('Coefficients', ascending=False)

coef_elasticnet_df

selected_features = list(X_train.columns[elasticnet.coef_ != 0])
X_train_elasticnet = X_train[selected_features]
X_test_elasticnet = X_test[selected_features]

svm = SVC()
svm.fit(X_train_elasticnet, y_train)
svm_score = svm.score(X_test_elasticnet, y_test)
print("SVM accuracy with", np.shape(selected_features)[0], "features given by ElasticNet:", svm_score)

lr = LogisticRegression(max_iter=10000)
lr.fit(X_train_lasso, y_train)
lr_score = lr.score(X_test_lasso, y_test)
print("Logistic Regression accuracy with", np.shape(selected_features)[0], "features given by ElasticNet:", lr_score)

for k in n_features:
    top_k_features = coef_sorted[:k].index.tolist()
    selected_features_train = X_train[top_k_features] 
    selected_features_test = X_test[top_k_features]
    svm = SVC()
    svm.fit(selected_features_train, y_train)
    svm_score = svm.score(selected_features_test, y_test)
    print("SVM accuracy with", k, "features:", svm_score)

print()
print("###############")
print()

for k in n_features:
    top_k_features = coef_sorted[:k].index.tolist()
    selected_features_train = X_train[top_k_features]
    selected_features_test = X_test[top_k_features]
    lr = LogisticRegression(max_iter=10000)
    lr.fit(selected_features_train, y_train)
    lr_score = lr.score(selected_features_test, y_test)
    print("Logistic Regression accuracy with", k, "features:", lr_score)

"""**Select *k*-best**"""

selector = SelectKBest(f_classif, k='all')
selector.fit(X_train, y_train)
scores = -np.log10(selector.pvalues_)
scores /= scores.max()
coef = pd.Series(scores, index=X_pca.columns)
coef_sorted = coef.sort_values(ascending=False)

coef_df = scores
coef_bestk_df = pd.DataFrame({'Feature Name': X_pca.columns, 'Coefficients': coef_df})
coef_bestk_df = coef_bestk_df.sort_values('Coefficients', ascending=False)

coef_bestk_df

for k in n_features:
    top_k_features = coef_sorted[:k].index.tolist()
    selected_features_train = X_train[top_k_features]
    selected_features_test = X_test[top_k_features]
    svm = SVC()
    svm.fit(selected_features_train, y_train)
    svm_score = svm.score(selected_features_test, y_test)
    print("SVM accuracy with", k, "features:", svm_score)

print()
print("###############")
print()

for k in n_features:
    top_k_features = coef_sorted[:k].index.tolist()
    selected_features_train = X_train[top_k_features]
    selected_features_test = X_test[top_k_features]
    lr = LogisticRegression(max_iter=10000)
    lr.fit(selected_features_train, y_train)
    lr_score = lr.score(selected_features_test, y_test)
    print("Logistic Regression accuracy with", k, "features:", lr_score)

"""# New section"""